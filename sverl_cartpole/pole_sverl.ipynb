{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Defines RL environments\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (4,4)  # Set size of visualization\n",
    "from IPython.display import clear_output  # For inline visualization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cma\n",
    "from math import prod\n",
    "\n",
    "# Define task\n",
    "env = gym.make('CartPole-v1')\n",
    "state_space_dimension = env.observation_space.shape[0]\n",
    "action_space_dimension = 1  # env.action_space.n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_space_dimension, action_space_dimension, num_neurons=5, bias = False):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc = nn.Linear(state_space_dimension, num_neurons, bias=bias)\n",
    "        self.fc1 = nn.Linear(num_neurons, action_space_dimension, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.tanh(self.fc(x))\n",
    "        output = self.fc1(hidden)\n",
    "        return output\n",
    "\n",
    "policy_net = Policy(state_space_dimension, action_space_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This just comes from the CMA assignment\n",
    "def fitness_cart_pole(x, nn, env):\n",
    "    '''\n",
    "    Returns negative accumulated reward for single pole, fully environment.\n",
    "\n",
    "    Parameters:\n",
    "        x: Parameter vector encoding the weights.\n",
    "        nn: Parameterized model.\n",
    "        env: Environment ('CartPole-v?').\n",
    "    '''\n",
    "    torch.nn.utils.vector_to_parameters(torch.Tensor(x), nn.parameters())  # Set the policy parameters\n",
    "    \n",
    "    state = env.reset()  # Forget about previous episode\n",
    "    state_tensor = torch.Tensor( state[0].reshape((1, state_space_dimension)) )\n",
    "          \n",
    "    R = 0  # Accumulated reward\n",
    "    while True:\n",
    "        out = nn(state_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env.step(a)  # Simulate pole\n",
    "        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\n",
    "        R += reward  # Accumulate \n",
    "        if truncated:\n",
    "            return -1000  # Episode ended, final goal reached, we consider minimization\n",
    "        if terminated:\n",
    "            return -R  # Episode ended, we consider minimization\n",
    "    return -R  # Never reached  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def step_policy(policy_net):\\n    trajectory_features = []  # Store the features of the trajectory\\n    env_render = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=5000)\\n    state = env_render.reset()  # Forget about previous episode\\n    state_tensor = torch.Tensor( state[0].reshape((1, state_space_dimension)) )\\n    steps = 0\\n    while True:\\n        out = policy_net(state_tensor)\\n        a = int(out > 0)\\n        state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\\n        steps+=1\\n        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\\n        trajectory_features.append(state_tensor.detach().numpy()[0])\\n        if(terminated or truncated): \\n            break\\n        \\n    env_render.close()\\n    return steps, trajectory_features  # Return the number of steps and the trajectory features \""
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only used for evaluating the model performance\n",
    "def step_policy(policy_net):\n",
    "    trajectory_features = []  # Store the features of the trajectory\n",
    "    env_render = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=5000)\n",
    "    state = env_render.reset()  # Forget about previous episode\n",
    "    state_tensor = torch.Tensor( state[0].reshape((1, state_space_dimension)) )\n",
    "    steps = 0\n",
    "    while True:\n",
    "        out = policy_net(state_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\n",
    "        steps+=1\n",
    "        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\n",
    "        trajectory_features.append(state_tensor.detach().numpy()[0])\n",
    "        if(terminated or truncated): \n",
    "            break\n",
    "        \n",
    "    env_render.close()\n",
    "    return steps, trajectory_features  # Return the number of steps and the trajectory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, just comes from the assignment\n",
    "def train_agent():     \n",
    "    policy_net = Policy(state_space_dimension, action_space_dimension)\n",
    "    d = sum(param.numel() for param in policy_net.parameters())\n",
    "    initial_weights = np.random.normal(0, 0.01, d)  # Random parameters for initial policy, d denotes the number of weights\n",
    "    initial_sigma = .01 # Initial global step-size sigma\n",
    "    # Do the optimization\n",
    "    res = cma.fmin(fitness_cart_pole,  # Objective function\n",
    "                initial_weights,  # Initial search point\n",
    "                initial_sigma,  # Initial global step-size sigma\n",
    "                args=([policy_net, env]),  # Arguments passed to the fitness function\n",
    "                options={'ftarget': -9999.9, 'tolflatfitness':1000, 'eval_final_mean':False})\n",
    "    env.close()\n",
    "  \n",
    "    # Set the policy parameters to the final solution\n",
    "    torch.nn.utils.vector_to_parameters(torch.Tensor(res[0]), policy_net.parameters())      \n",
    "\n",
    "    return policy_net, res[2]  # Return the number of steps and the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(policy_net, no_runs = 10): \n",
    "    pole_bal_unbiased = []\n",
    "    trajectories = []  # Store the features of the trajectories\n",
    "    \n",
    "    for _ in range(no_runs):     \n",
    "\n",
    "        steps, trajectory_features = step_policy(policy_net)\n",
    "        \n",
    "        pole_bal_unbiased.append(steps)  # Store the number of steps in the pole balancing task\n",
    "        trajectories+=trajectory_features\n",
    "        \n",
    "    return pole_bal_unbiased, np.array(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6_w,13)-aCMA-ES (mu_w=4.0,w_1=38%) in dimension 25 (seed=461772, Mon Mar 31 13:54:36 2025)\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1     13 -2.020000000000000e+02 1.0e+00 9.27e-03  9e-03  9e-03 0:00.1\n",
      "    2     26 -4.660000000000000e+02 1.1e+00 9.19e-03  9e-03  9e-03 0:00.2\n",
      "    3     39 -1.000000000000000e+03 1.1e+00 9.08e-03  9e-03  9e-03 0:00.3\n",
      "   14    182 -1.000000000000000e+03 1.4e+00 8.51e-03  8e-03  9e-03 0:03.7\n",
      "   27    351 -1.000000000000000e+03 1.7e+00 8.66e-03  8e-03  9e-03 0:07.9\n",
      "   41    533 -1.000000000000000e+03 1.8e+00 8.38e-03  8e-03  9e-03 0:13.0\n",
      "   55    715 -1.000000000000000e+03 2.1e+00 9.76e-03  9e-03  1e-02 0:19.2\n",
      "   71    923 -1.000000000000000e+03 2.3e+00 8.60e-03  8e-03  1e-02 0:26.4\n",
      "termination on tolfun=1e-11 (Mon Mar 31 13:55:02 2025)\n",
      "termination on tolfunhist=1e-12 (Mon Mar 31 13:55:02 2025)\n",
      "final/bestever f-value = -1.000000e+03 -1.000000e+03 after 923/29 evaluations\n",
      "incumbent solution: [-0.03743545 -0.04255964  0.0753081   0.06903252 -0.03464984 -0.03518623\n",
      " -0.0932536  -0.10417882 ...]\n",
      "std deviations: [0.00845072 0.00831705 0.00873971 0.0088601  0.0080145  0.00841601\n",
      " 0.00909272 0.00950612 ...]\n"
     ]
    }
   ],
   "source": [
    "policy_net, eval_steps = train_agent() #Here we train the agent, and report the evaluation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pole_bal_unbiased, trajectories = get_performance(policy_net, 20) #Running the agent for 20 times, and storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of evaluations is:  29\n",
      "The average number of steps with max_steps = 5000, is:  491.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The average number of steps with max_steps = 5000, is: \", np.mean(pole_bal_unbiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Neural Conditioner, which comes from this paper: https://arxiv.org/pdf/1902.08401\n",
    "#It is a basically a variational autoencoder with GAN-like training, where the generator is a neural conditioner, and the discriminator is a neural network that tries to distinguish between real and fake data.\n",
    "class NeuralConditioner(nn.Module):\n",
    "    def __init__(self, input_dim=4, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: [x_a (4) + a (4) + r (4)] = 12 dim input\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder: [z (latent_dim) + x_a (4) + a (4) + r (4)] = latent_dim + 12\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_a, a, r, z):\n",
    "        # Input shapes should be: [batch_size, 4] for x_a, a, r\n",
    "        # z shape: [batch_size, latent_dim]\n",
    "        encoder_input = torch.cat([x_a, a, r], dim=1)\n",
    "        h = self.encoder(encoder_input)\n",
    "        \n",
    "        decoder_input = torch.cat([h, x_a, a, r], dim=1)  # Use encoded h instead of raw z\n",
    "        return self.decoder(decoder_input) * r\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=4):\n",
    "        super().__init__()\n",
    "        # Input: [x_r (4) + x_a (4) + a (4) + r (4)] = 16 dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_r, x_a, a, r):\n",
    "        inputs = torch.cat([x_r, x_a, a, r], dim=1)\n",
    "        return self.net(inputs)\n",
    "\n",
    "#Training Function\n",
    "def train_nc(nc, discriminator, dataloader, epochs):\n",
    "    nc.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    opt_nc = torch.optim.Adam(nc.parameters(), lr=1e-4)\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x_real in dataloader:\n",
    "            batch_size = x_real.size(0)\n",
    "            \n",
    "            # Create random masks\n",
    "            a = torch.zeros_like(x_real)\n",
    "            r = torch.zeros_like(x_real)\n",
    "            \n",
    "            # Ensure at least 1 feature observed and 1 predicted\n",
    "            for i in range(batch_size):\n",
    "                obs_idx = torch.randperm(4)[:torch.randint(1, 4, (1,))]\n",
    "                a[i, obs_idx] = 1\n",
    "                r[i, :] = 1 - a[i, :]\n",
    "                if r[i].sum() == 0:  # Ensure at least 1 predicted\n",
    "                    r[i, torch.randint(0, 4, (1,))] = 1\n",
    "            \n",
    "            # Generate samples\n",
    "            z = torch.randn(batch_size, nc.latent_dim)\n",
    "            x_a = x_real * a\n",
    "            x_r_fake = nc(x_a, a, r, z)\n",
    "            x_r_real = x_real * r\n",
    "            \n",
    "            # Train discriminator\n",
    "            opt_d.zero_grad()\n",
    "            d_real = discriminator(x_r_real, x_a, a, r)\n",
    "            d_fake = discriminator(x_r_fake.detach(), x_a, a, r)\n",
    "            loss_d = - (torch.log(d_real) + torch.log(1 - d_fake)).mean()\n",
    "            loss_d.backward()\n",
    "            opt_d.step()\n",
    "            \n",
    "            # Train generator\n",
    "            opt_nc.zero_grad()\n",
    "            d_fake = discriminator(x_r_fake, x_a, a, r)\n",
    "            loss_g = - torch.log(d_fake).mean()\n",
    "            loss_g.backward()\n",
    "            opt_nc.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | G Loss: {loss_g.item():.4f} | D Loss: {loss_d.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateFeatureDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data)  # Convert to PyTorch tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Example data (replace with your actual array)\n",
    "\n",
    "dataset = StateFeatureDataset(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "input_dim = 4  # Check the shape of the data\n",
    "\n",
    "latent_dim = 64  # Size of the latent space\n",
    "NC = NeuralConditioner(input_dim, latent_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | G Loss: 0.7034 | D Loss: 1.3972\n",
      "Epoch 2/50 | G Loss: 0.7078 | D Loss: 1.3833\n",
      "Epoch 3/50 | G Loss: 0.6895 | D Loss: 1.3859\n",
      "Epoch 4/50 | G Loss: 0.7033 | D Loss: 1.3487\n",
      "Epoch 5/50 | G Loss: 0.7033 | D Loss: 1.3486\n",
      "Epoch 6/50 | G Loss: 0.7015 | D Loss: 1.3871\n",
      "Epoch 7/50 | G Loss: 0.7018 | D Loss: 1.3809\n",
      "Epoch 8/50 | G Loss: 0.6522 | D Loss: 1.5016\n",
      "Epoch 9/50 | G Loss: 0.6855 | D Loss: 1.3806\n",
      "Epoch 10/50 | G Loss: 0.6985 | D Loss: 1.3786\n",
      "Epoch 11/50 | G Loss: 0.6957 | D Loss: 1.3645\n",
      "Epoch 12/50 | G Loss: 0.7055 | D Loss: 1.3374\n",
      "Epoch 13/50 | G Loss: 0.6899 | D Loss: 1.3892\n",
      "Epoch 14/50 | G Loss: 0.7109 | D Loss: 1.3610\n",
      "Epoch 15/50 | G Loss: 0.7085 | D Loss: 1.3664\n",
      "Epoch 16/50 | G Loss: 0.6910 | D Loss: 1.3777\n",
      "Epoch 17/50 | G Loss: 0.7017 | D Loss: 1.3782\n",
      "Epoch 18/50 | G Loss: 0.6988 | D Loss: 1.3760\n",
      "Epoch 19/50 | G Loss: 0.7164 | D Loss: 1.3233\n",
      "Epoch 20/50 | G Loss: 0.6968 | D Loss: 1.3830\n",
      "Epoch 21/50 | G Loss: 0.6822 | D Loss: 1.3955\n",
      "Epoch 22/50 | G Loss: 0.7023 | D Loss: 1.3755\n",
      "Epoch 23/50 | G Loss: 0.7161 | D Loss: 1.3737\n",
      "Epoch 24/50 | G Loss: 0.6968 | D Loss: 1.4228\n",
      "Epoch 25/50 | G Loss: 0.6930 | D Loss: 1.3806\n",
      "Epoch 26/50 | G Loss: 0.6990 | D Loss: 1.3768\n",
      "Epoch 27/50 | G Loss: 0.6961 | D Loss: 1.3758\n",
      "Epoch 28/50 | G Loss: 0.7459 | D Loss: 1.3595\n",
      "Epoch 29/50 | G Loss: 0.7066 | D Loss: 1.3647\n",
      "Epoch 30/50 | G Loss: 0.7391 | D Loss: 1.3545\n",
      "Epoch 31/50 | G Loss: 0.6939 | D Loss: 1.3314\n",
      "Epoch 32/50 | G Loss: 0.7134 | D Loss: 1.3916\n",
      "Epoch 33/50 | G Loss: 0.7189 | D Loss: 1.3361\n",
      "Epoch 34/50 | G Loss: 0.7142 | D Loss: 1.3391\n",
      "Epoch 35/50 | G Loss: 0.7373 | D Loss: 1.3444\n",
      "Epoch 36/50 | G Loss: 0.6845 | D Loss: 1.3900\n",
      "Epoch 37/50 | G Loss: 0.7033 | D Loss: 1.3801\n",
      "Epoch 38/50 | G Loss: 0.6805 | D Loss: 1.3697\n",
      "Epoch 39/50 | G Loss: 0.6864 | D Loss: 1.3653\n",
      "Epoch 40/50 | G Loss: 0.7009 | D Loss: 1.3885\n",
      "Epoch 41/50 | G Loss: 0.7161 | D Loss: 1.3789\n",
      "Epoch 42/50 | G Loss: 0.6986 | D Loss: 1.3843\n",
      "Epoch 43/50 | G Loss: 0.7190 | D Loss: 1.3675\n",
      "Epoch 44/50 | G Loss: 0.7036 | D Loss: 1.3640\n",
      "Epoch 45/50 | G Loss: 0.6881 | D Loss: 1.3912\n",
      "Epoch 46/50 | G Loss: 0.6882 | D Loss: 1.4018\n",
      "Epoch 47/50 | G Loss: 0.6989 | D Loss: 1.3711\n",
      "Epoch 48/50 | G Loss: 0.7046 | D Loss: 1.4063\n",
      "Epoch 49/50 | G Loss: 0.6915 | D Loss: 1.3854\n",
      "Epoch 50/50 | G Loss: 0.7262 | D Loss: 1.3658\n"
     ]
    }
   ],
   "source": [
    "train_nc(NC, discriminator, dataloader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True values:  [ 0.01735522  0.23706736  0.03950975 -0.28725505]\n",
      "Updated features:  [ 0.01735522  0.27326572  0.03950975 -0.28725505]\n"
     ]
    }
   ],
   "source": [
    "def predict_missing_features(nc, observed_features, observed_mask):\n",
    "    \"\"\"\n",
    "    observed_features: Array of shape (4,), with NaN for missing features.\n",
    "    observed_mask: Binary array (1 = observed, 0 = missing).\n",
    "    \"\"\"\n",
    "    # Convert to PyTorch tensors\n",
    "    x_a = torch.FloatTensor(np.nan_to_num(observed_features, nan=0.0) * observed_mask)\n",
    "    a = torch.FloatTensor(observed_mask)\n",
    "    r = 1 - a  # Predict missing features\n",
    "    \n",
    "    # Generate predictions (multiple samples for uncertainty)\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(nc.latent_dim)\n",
    "        x_a = x_a.unsqueeze(0)\n",
    "        a = a.unsqueeze(0)\n",
    "        r = r.unsqueeze(0)\n",
    "        preds = nc(x_a, a, r, z)\n",
    "    \n",
    "    return preds.mean(0).numpy()\n",
    "\n",
    "# Example usage:\n",
    "observed_features = np.array([ 0.01735522 , 0.23706736 , 0.03950975 ,-0.28725505])\n",
    "observed_mask = np.array([1, 0, 1, 1])  # 1 = observed, 0 = missing\n",
    "\n",
    "mean_pred = predict_missing_features(NC, observed_features, observed_mask)\n",
    "\n",
    "print(\"True values: \", observed_features)\n",
    "\n",
    "for i in range(4):\n",
    "    if observed_mask[i] == 0:\n",
    "        observed_features[i] = mean_pred[i]\n",
    "\n",
    "print(\"Updated features: \", observed_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_from_state(policy_net, seed, believed_initial_state):\n",
    "    ''''\n",
    "    'Evaluate the policy from a given state, using the believed state to make the initial decisino'\n",
    "    '''\n",
    "    steps = 0\n",
    "    env_render = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=5000)\n",
    "    true_state = env_render.reset(seed=seed)  # Forget about previous episode\n",
    "    believed_tensor = torch.Tensor( believed_initial_state.reshape((1, state_space_dimension)) )\n",
    "    true_tensor = torch.Tensor( true_state[0].reshape((1, state_space_dimension)) )\n",
    "\n",
    "\n",
    "    print(\"Believed state: \", believed_tensor)\n",
    "    print(\"True state: \", true_tensor)\n",
    "\n",
    "    print(\"Action if evaluated on true state: \", int(policy_net(true_tensor)>0))\n",
    "    print(\"Action if evaluated on believed state: \", int(policy_net(believed_tensor)>0))\n",
    "\n",
    "    out = policy_net(believed_tensor)\n",
    "    a = int(out > 0)\n",
    "    state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\n",
    "    steps +=1\n",
    "    state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) ) \n",
    "    \n",
    "\n",
    "    while True:\n",
    "        out = policy_net(state_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\n",
    "        steps+=1\n",
    "        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\n",
    "        if(terminated or truncated): \n",
    "            break\n",
    "        \n",
    "    env_render.close()\n",
    "    return steps  # Return the number of steps and the trajectory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_missing_features(policy_net, seed, NC, mask):\n",
    "    ''''\n",
    "    'Evaluate the policy from a given state, using the believed state to make the initial decisino'\n",
    "    '''\n",
    "    steps = 0\n",
    "    env_render = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=5000)\n",
    "    true_state = env_render.reset(seed=seed)  # Forget about previous episode\n",
    "    \n",
    "    pred = predict_missing_features(NC, true_state[0].reshape((1, state_space_dimension))[0], mask)\n",
    "\n",
    "\n",
    "    believed_state = np.copy(true_state[0].reshape((1, state_space_dimension)))[0]\n",
    "\n",
    "    for i in range(4):\n",
    "        if mask[i] == 0:\n",
    "            believed_state[i] = pred[i]\n",
    "\n",
    "    believed_tensor = torch.Tensor( believed_state)\n",
    "\n",
    "    while True:       \n",
    "\n",
    "        out = policy_net(believed_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\n",
    "        steps+=1\n",
    "        pred = predict_missing_features(NC, state.reshape((1, state_space_dimension))[0], mask)\n",
    "        believed_state = np.copy(state.reshape((1, state_space_dimension)))[0]\n",
    "        for i in range(4):\n",
    "            if mask[i] == 0:\n",
    "                believed_state[i] = pred[i]\n",
    "\n",
    "        believed_tensor = torch.Tensor( believed_state)\n",
    "        if(terminated or truncated): \n",
    "            break\n",
    "        \n",
    "    env_render.close()\n",
    "    return steps  # Return the number of steps and the trajectory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_gain(i, C): \n",
    "    C_i = np.copy(C)\n",
    "    C_i[i] = 1\n",
    "\n",
    "    if(np.sum(C) == 0): \n",
    "        V_C = 0\n",
    "    else: \n",
    "        V_C = eval_with_missing_features(policy_net, 0, NC, C)\n",
    "\n",
    "    \n",
    "    V_C_i = eval_with_missing_features(policy_net, 0, NC, C_i)\n",
    "    return V_C_i - V_C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subsets(feature):\n",
    "    variations = []\n",
    "    # The i-th position is fixed to 0, so we'll generate all combinations for other positions\n",
    "    # There are n-1 positions that can vary (each can be 0 or 1), so total 2^(n-1) variations\n",
    "    for num in range(8):\n",
    "        binary = []\n",
    "        # We'll build the binary list, inserting 0 at position i\n",
    "        # and filling the rest based on the binary representation of 'num'\n",
    "        # We need to split 'num' into bits for positions before and after i\n",
    "        # Initialize a counter for the current bit position\n",
    "        bit_pos = 0\n",
    "        for pos in range(4):\n",
    "            if pos == feature:\n",
    "                binary.append(0)\n",
    "            else:\n",
    "                # Get the bit at position 'bit_pos' from 'num'\n",
    "                bit = (num >> (4 - 2 - bit_pos)) & 1\n",
    "                binary.append(bit)\n",
    "                bit_pos += 1\n",
    "        variations.append(binary)\n",
    "    return variations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapley_value(feature):\n",
    "    list_of_C = get_all_subsets(feature)\n",
    "    sum = 0\n",
    "\n",
    "    for C in list_of_C:\n",
    "        sum += marginal_gain(feature, C)* (prod(range(1, np.sum(C)+1))*prod(range(1, 4 - np.sum(C))) /prod(range(1, 5)))\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.83333333333333"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapley_value(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE TO EXPLAIN:  [[ 0.01369617 -0.02302133 -0.04590265 -0.04834723]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_render = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=200)\n",
    "env_render.reset(seed = 0)  # Forget about previous episode\n",
    "state = env_render.reset(seed=0)[0].reshape((1, state_space_dimension))\n",
    "print(\"STATE TO EXPLAIN: \", state)\n",
    "state_tensor = torch.Tensor( state )\n",
    "plt.imshow(env_render.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapley value of Cart Position:  49.66666666666667\n",
      "Shapley value of Cart Velocity:  13.333333333333332\n",
      "Shapley value of Pole Angle:  5.166666666666667\n",
      "Shapley value of Pole Angular Velocity:  85.83333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapley value of Cart Position: \", shapley_value(0))\n",
    "print(\"Shapley value of Cart Velocity: \", shapley_value(1))\n",
    "print(\"Shapley value of Pole Angle: \", shapley_value(2))\n",
    "print(\"Shapley value of Pole Angular Velocity: \", shapley_value(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
