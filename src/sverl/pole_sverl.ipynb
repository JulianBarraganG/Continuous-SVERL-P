{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Defines RL environments\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (4,4)  # Set size of visualization\n",
    "from IPython.display import clear_output  # For inline visualization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cma\n",
    "from math import factorial\n",
    "\n",
    "# Define task\n",
    "env = gym.make('CartPole-v1')\n",
    "state_space_dimension = env.observation_space.shape[0]\n",
    "action_space_dimension = 1  # env.action_space.n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_space_dimension, action_space_dimension, num_neurons=5, bias = False):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc = nn.Linear(state_space_dimension, num_neurons, bias=bias)\n",
    "        self.fc1 = nn.Linear(num_neurons, action_space_dimension, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.tanh(self.fc(x))\n",
    "        output = self.fc1(hidden)\n",
    "        return output\n",
    "\n",
    "policy_net = Policy(state_space_dimension, action_space_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This just comes from the CMA assignment\n",
    "def fitness_cart_pole(x, nn, env):\n",
    "    '''\n",
    "    Returns negative accumulated reward for single pole, fully environment.\n",
    "\n",
    "    Parameters:\n",
    "        x: Parameter vector encoding the weights.\n",
    "        nn: Parameterized model.\n",
    "        env: Environment ('CartPole-v?').\n",
    "    '''\n",
    "    torch.nn.utils.vector_to_parameters(torch.Tensor(x), nn.parameters())  # Set the policy parameters\n",
    "    \n",
    "    state = env.reset()  # Forget about previous episode\n",
    "    state_tensor = torch.Tensor( state[0].reshape((1, state_space_dimension)) )\n",
    "          \n",
    "    R = 0  # Accumulated reward\n",
    "    while True:\n",
    "        out = nn(state_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env.step(a)  # Simulate pole\n",
    "        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\n",
    "        R += reward  # Accumulate \n",
    "        if truncated:\n",
    "            return -1000  # Episode ended, final goal reached, we consider minimization\n",
    "        if terminated:\n",
    "            return -R  # Episode ended, we consider minimization\n",
    "    return -R  # Never reached  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only used for evaluating the model performance\n",
    "def one_run(policy_net, env):\n",
    "    trajectory_features = []  # Store the features of the trajectory\n",
    "    state = env.reset()  # Forget about previous episode\n",
    "    state_tensor = torch.Tensor( state[0].reshape((1, state_space_dimension)) )\n",
    "    steps = 0\n",
    "    while True:\n",
    "        out = policy_net(state_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env.step(a)  # Simulate pole\n",
    "        steps+=1\n",
    "        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\n",
    "        trajectory_features.append(state_tensor.detach().numpy()[0])\n",
    "        if(terminated or truncated): \n",
    "            break\n",
    "        \n",
    "    env.close()\n",
    "    return steps, trajectory_features  # Return the number of steps and the trajectory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, just comes from the assignment\n",
    "def train_agent(policy_net , env):     \n",
    "    d = sum(param.numel() for param in policy_net.parameters())\n",
    "    initial_weights = np.random.normal(0, 0.01, d)  # Random parameters for initial policy, d denotes the number of weights\n",
    "    initial_sigma = .01 # Initial global step-size sigma\n",
    "    # Do the optimization\n",
    "    res = cma.fmin(fitness_cart_pole,  # Objective function\n",
    "                initial_weights,  # Initial search point\n",
    "                initial_sigma,  # Initial global step-size sigma\n",
    "                args=([policy_net, env]),  # Arguments passed to the fitness function\n",
    "                options={'ftarget': -9999.9, 'tolflatfitness':1000, 'eval_final_mean':False})\n",
    "    env.close()\n",
    "  \n",
    "    # Set the policy parameters to the final solution\n",
    "    torch.nn.utils.vector_to_parameters(torch.Tensor(res[0]), policy_net.parameters())      \n",
    "\n",
    "    return policy_net  # Return the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(policy_net, env, no_runs = 10): \n",
    "    pole_bal = []\n",
    "    trajectories = []  # Store the features of the trajectories\n",
    "    \n",
    "    for _ in range(no_runs):     \n",
    "\n",
    "        steps, trajectory_features = one_run(policy_net, env)        \n",
    "        pole_bal.append(steps)  # Store the number of steps in the pole balancing task\n",
    "        trajectories+=trajectory_features\n",
    "        \n",
    "    return pole_bal, np.array(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "policy_net = train_agent(Policy(state_space_dimension, action_space_dimension), gym.make('CartPole-v1')) #Here we train the agent, and report the evaluation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pole_bal, trajectories = get_performance(policy_net, no_runs = 20) #Running the agent for 20 times, and storing the results\n",
    "#Also storing the trajectories, which is used to train the Neural Conditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average number of steps with max_steps = 5000, is: \", np.mean(pole_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Neural Conditioner, which comes from this paper: https://arxiv.org/pdf/1902.08401\n",
    "#It is a basically a variational autoencoder with GAN-like training, where the generator is a neural conditioner, and the discriminator is a neural network that tries to distinguish between real and fake data.\n",
    "class NeuralConditioner(nn.Module):\n",
    "    def __init__(self, input_dim=4, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: [x_a (4) + a (4) + r (4)] = 12 dim input\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder: [z (latent_dim) + x_a (4) + a (4) + r (4)] = latent_dim + 12\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_a, a, r, z):\n",
    "        # Input shapes should be: [batch_size, 4] for x_a, a, r\n",
    "        # z shape: [batch_size, latent_dim]\n",
    "        encoder_input = torch.cat([x_a, a, r], dim=1)\n",
    "        h = self.encoder(encoder_input)\n",
    "        \n",
    "        decoder_input = torch.cat([h, x_a, a, r], dim=1)  # Use encoded h instead of raw z\n",
    "        return self.decoder(decoder_input) * r\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=4):\n",
    "        super().__init__()\n",
    "        # Input: [x_r (4) + x_a (4) + a (4) + r (4)] = 16 dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_r, x_a, a, r):\n",
    "        inputs = torch.cat([x_r, x_a, a, r], dim=1)\n",
    "        return self.net(inputs)\n",
    "\n",
    "#Training Function\n",
    "def train_nc(nc, discriminator, dataloader, epochs):\n",
    "    nc.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    opt_nc = torch.optim.Adam(nc.parameters(), lr=1e-4)\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x_real in dataloader:\n",
    "            batch_size = x_real.size(0)\n",
    "            \n",
    "            # Create random masks\n",
    "            a = torch.zeros_like(x_real)\n",
    "            r = torch.zeros_like(x_real)\n",
    "            \n",
    "            # Ensure at least 1 feature observed and 1 predicted\n",
    "            for i in range(batch_size):\n",
    "                obs_idx = torch.randperm(4)[:torch.randint(1, 4, (1,))]\n",
    "                a[i, obs_idx] = 1\n",
    "                r[i, :] = 1 - a[i, :]\n",
    "                if r[i].sum() == 0:  # Ensure at least 1 predicted\n",
    "                    r[i, torch.randint(0, 4, (1,))] = 1\n",
    "            \n",
    "            # Generate samples\n",
    "            z = torch.randn(batch_size, nc.latent_dim)\n",
    "            x_a = x_real * a\n",
    "            x_r_fake = nc(x_a, a, r, z)\n",
    "            x_r_real = x_real * r\n",
    "            \n",
    "            # Train discriminator\n",
    "            opt_d.zero_grad()\n",
    "            d_real = discriminator(x_r_real, x_a, a, r)\n",
    "            d_fake = discriminator(x_r_fake.detach(), x_a, a, r)\n",
    "            loss_d = - (torch.log(d_real) + torch.log(1 - d_fake)).mean()\n",
    "            loss_d.backward()\n",
    "            opt_d.step()\n",
    "            \n",
    "            # Train generator\n",
    "            opt_nc.zero_grad()\n",
    "            d_fake = discriminator(x_r_fake, x_a, a, r)\n",
    "            loss_g = - torch.log(d_fake).mean()\n",
    "            loss_g.backward()\n",
    "            opt_nc.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | G Loss: {loss_g.item():.4f} | D Loss: {loss_d.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateFeatureDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data)  # Convert to PyTorch tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "#Convert the trajectories to a PyTorch dataset, such that they can be used to train the Neural Conditioner\n",
    "dataset = StateFeatureDataset(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "input_dim = 4  # size of the data\n",
    "\n",
    "latent_dim = 64  # Size of the latent space\n",
    "NC = NeuralConditioner(input_dim, latent_dim)\n",
    "discriminator = Discriminator(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nc(NC, discriminator, dataloader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict missing features using the trained Neural Conditioner\n",
    "#Takes the neural conditioner, all features (where missing features can be np.nan, but can also have their true values which will just be ignored), \n",
    "#and a mask that indicates which features are missing (1 = observed, 0 = missing)\n",
    "def predict_missing_features(nc, observed_features, observed_mask):\n",
    "    \"\"\"\n",
    "    observed_features: Array of shape (4,), with NaN for missing features.\n",
    "    observed_mask: Binary array (1 = observed, 0 = missing).\n",
    "    \"\"\"\n",
    "    # Convert to PyTorch tensors\n",
    "    x_a = torch.FloatTensor(np.nan_to_num(observed_features, nan=0.0) * observed_mask)\n",
    "    a = torch.FloatTensor(observed_mask)\n",
    "    r = 1 - a  # Predict missing features\n",
    "    \n",
    "    # Generate predictions (multiple samples for uncertainty)\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(nc.latent_dim)\n",
    "        x_a = x_a.unsqueeze(0)\n",
    "        a = a.unsqueeze(0)\n",
    "        r = r.unsqueeze(0)\n",
    "        preds = nc(x_a, a, r, z)\n",
    "    \n",
    "    return preds.mean(0).numpy()\n",
    "\n",
    "# Example usage:\n",
    "observed_features = np.array([ 0.01735522 , 0.23706736 , 0.03950975 ,-0.28725505])\n",
    "mask = np.array([0, 1, 1, 1])  # 1 = observed, 0 = missing\n",
    "\n",
    "\n",
    "\n",
    "mean_pred = predict_missing_features(NC, observed_features, mask)\n",
    "\n",
    "print(\"True values: \", observed_features)\n",
    "\n",
    "for i in range(4):\n",
    "    if mask[i] == 0:\n",
    "        observed_features[i] = mean_pred[i]\n",
    "\n",
    "print(\"Predicted features: \", observed_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basically the local sverl. Uses the neural conditioner to predict missing features in the first step, and then has full observability afterwards. \n",
    "#Very uninteresting to be honest, since the cart pole can only go left 0, or right 1. And even though the model gives different values, the decision\n",
    "#Will usually still be the same, just with more or less certainty. And even if the missing features leads to a bad decision \n",
    "#In the initital step, it can be saved, so it doesn't really matter much. \n",
    "#I haven't used this function much\n",
    "def eval_from_state(policy_net, seed, believed_initial_state):\n",
    "    ''''\n",
    "    'Evaluate the policy from a given state, using the believed state to make the initial decision'\n",
    "    '''\n",
    "    steps = 0\n",
    "    env_render = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=5000)\n",
    "    true_state = env_render.reset(seed=seed)  # Forget about previous episode\n",
    "    believed_tensor = torch.Tensor( believed_initial_state.reshape((1, state_space_dimension)) )\n",
    "    true_tensor = torch.Tensor( true_state[0].reshape((1, state_space_dimension)) )\n",
    "\n",
    "\n",
    "    print(\"Believed state: \", believed_tensor)\n",
    "    print(\"True state: \", true_tensor)\n",
    "\n",
    "    print(\"Action if evaluated on true state: \", int(policy_net(true_tensor)>0))\n",
    "    print(\"Action if evaluated on believed state: \", int(policy_net(believed_tensor)>0))\n",
    "\n",
    "    out = policy_net(believed_tensor)\n",
    "    a = int(out > 0)\n",
    "    state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\n",
    "    steps +=1\n",
    "    state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) ) \n",
    "    \n",
    "\n",
    "    while True:\n",
    "        out = policy_net(state_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\n",
    "        steps+=1\n",
    "        state_tensor = torch.Tensor( state.reshape((1, state_space_dimension)) )\n",
    "        if(terminated or truncated): \n",
    "            break\n",
    "        \n",
    "    env_render.close()\n",
    "    return steps  # Return the number of steps and the trajectory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now this is juicy. Gives a global evaluation of the policy, but with missing features.\n",
    "#In every step, it uses the neural conditioner to predict the missing features, and then uses the policy to decide what to do.\n",
    "\n",
    "def eval_with_missing_features(policy_net, seed, NC, mask):\n",
    "    ''''\n",
    "    'Evaluate the policy from a given state, using the believed state to make the initial decisino'\n",
    "    '''\n",
    "    steps = 0\n",
    "    env_render = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=5000)\n",
    "    true_state = env_render.reset(seed=seed)  # Forget about previous episode\n",
    "    \n",
    "    pred = predict_missing_features(NC, true_state[0].reshape((1, state_space_dimension))[0], mask)\n",
    "\n",
    "\n",
    "    believed_state = np.copy(true_state[0].reshape((1, state_space_dimension)))[0]\n",
    "\n",
    "    for i in range(4):\n",
    "        if mask[i] == 0:\n",
    "            believed_state[i] = pred[i]\n",
    "\n",
    "    believed_tensor = torch.Tensor( believed_state)\n",
    "\n",
    "    while True:       \n",
    "\n",
    "        out = policy_net(believed_tensor)\n",
    "        a = int(out > 0)\n",
    "        state, reward, terminated, truncated, _ = env_render.step(a)  # Simulate pole\n",
    "        steps+=1\n",
    "        pred = predict_missing_features(NC, state.reshape((1, state_space_dimension))[0], mask)\n",
    "        believed_state = np.copy(state.reshape((1, state_space_dimension)))[0]\n",
    "        for i in range(4):\n",
    "            if mask[i] == 0:\n",
    "                believed_state[i] = pred[i]\n",
    "\n",
    "        believed_tensor = torch.Tensor( believed_state)\n",
    "        if(terminated or truncated): \n",
    "            break\n",
    "        \n",
    "    env_render.close()\n",
    "    return steps  # Return the number of steps and the trajectory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a feature i, and a mask C. Gives the marginal gain of adding feature i to the mask C, via eval_with_missing_features.\n",
    "def marginal_gain(i, C, seed): \n",
    "    C_i = np.copy(C)\n",
    "    C_i[i] = 1\n",
    "\n",
    "    \"\"\" if(np.sum(C) == 0): \n",
    "        # v(Ø) = 0 for game theory, and for ML v(Ø) = expected prediction of model.\n",
    "        # We have yet to determine what v(Ø) is for SVERL, and for the general RL case.\n",
    "        V_C = 0\n",
    "    else: \n",
    "        V_C = eval_with_missing_features(policy_net, seed, NC, C) \"\"\"\n",
    "\n",
    "    \n",
    "    V_C = eval_with_missing_features(policy_net, seed, NC, C)\n",
    "    \n",
    "    V_C_i = eval_with_missing_features(policy_net, seed, NC, C_i)\n",
    "    return V_C_i - V_C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets all subsets of masks when one feature is fixed to 0.\n",
    "#Basically all C \\in F/i \n",
    "\n",
    "def get_all_subsets(feature):\n",
    "    variations = []\n",
    "    # The i-th position is fixed to 0, so we'll generate all combinations for other positions\n",
    "    # There are n-1 positions that can vary (each can be 0 or 1), so total 2^(n-1) variations\n",
    "    for num in range(8):\n",
    "        binary = []\n",
    "        # We'll build the binary list, inserting 0 at position i\n",
    "        # and filling the rest based on the binary representation of 'num'\n",
    "        # We need to split 'num' into bits for positions before and after i\n",
    "        # Initialize a counter for the current bit position\n",
    "        bit_pos = 0\n",
    "        for pos in range(4):\n",
    "            if pos == feature:\n",
    "                binary.append(0)\n",
    "            else:\n",
    "                # Get the bit at position 'bit_pos' from 'num'\n",
    "                bit = (num >> (4 - 2 - bit_pos)) & 1\n",
    "                binary.append(bit)\n",
    "                bit_pos += 1\n",
    "        variations.append(binary)\n",
    "    return variations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates Shapley values for a feature, using the marginal gain function and the get_all_subsets function.\n",
    "def shapley_value(feature, seed):\n",
    "    list_of_C = get_all_subsets(feature)\n",
    "    sum = 0\n",
    "\n",
    "    for C in list_of_C:\n",
    "        sum += marginal_gain(feature, C, seed)* ((factorial(np.sum(C))*factorial(4 - np.sum(C) - 1)) / factorial(4))\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the seed is set to 0, the results are always the same.\n",
    "#This is the state which we start in every time for the purpose of this experiment. To get proper shapley values, all this should probably be evaluated\n",
    "#many times with different seeds. \n",
    "env_render = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=5000)\n",
    "env_render.reset(seed = 0)  # Forget about previous episode\n",
    "state = env_render.reset(seed=0)[0].reshape((1, state_space_dimension))\n",
    "print(\"STATE TO EXPLAIN: \", state)\n",
    "state_tensor = torch.Tensor( state )\n",
    "plt.imshow(env_render.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The i is the seed. This is the only way I know how to set the starting position \n",
    "#We are doing 100 different seeds, and averaging the results.\n",
    "NUM_ROUNDS = 100\n",
    "shapley_cart_pos = 0\n",
    "shapley_cart_vel = 0\n",
    "shapley_pole_angle = 0\n",
    "shapley_pole_vel = 0\n",
    "for i in range(NUM_ROUNDS): \n",
    "    shapley_cart_pos += shapley_value(0, i)\n",
    "    shapley_cart_vel += shapley_value(1, i)\n",
    "    shapley_pole_angle += shapley_value(2, i)\n",
    "    shapley_pole_vel += shapley_value(3, i)\n",
    "shapley_cart_pos /= NUM_ROUNDS\n",
    "shapley_cart_vel /= NUM_ROUNDS\n",
    "shapley_pole_angle /= NUM_ROUNDS\n",
    "shapley_pole_vel /= NUM_ROUNDS\n",
    "print(\"Shapley value of Cart Position: \", shapley_cart_pos)\n",
    "print(\"Shapley value of Cart Velocity: \", shapley_cart_vel)\n",
    "print(\"Shapley value of Pole Angle: \", shapley_pole_angle)\n",
    "print(\"Shapley value of Pole Angular Velocity: \", shapley_pole_vel)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
