{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XRL Baby Steps Notebook\n",
    "\n",
    "In this notebook, we implement the baby steps attempt to combine three XRL methods,\n",
    "in order to explain deep RL.\n",
    "\n",
    "- SVERL\n",
    "- Group-SHAPLEY\n",
    "- Shapley Explainability on Data Manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"policy\": 'MlpPolicy',\n",
    "    \"n_steps\": 1024,\n",
    "    \"batch_size\": 64,\n",
    "    \"gae_lambda\": 0.98,\n",
    "    \"gamma\": 0.999,\n",
    "    \"n_epochs\": 4,\n",
    "    \"ent_coef\": 0.01,\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "n_envs = 16\n",
    "n_timesteps = 1e6\n",
    "save_freq = max(int(n_timesteps / 10), 1000)\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create vectorized environment\n",
    "env = make_vec_env(\n",
    "    \"LunarLander-v3\", \n",
    "    n_envs=n_envs, \n",
    "    vec_env_cls=SubprocVecEnv,\n",
    "    monitor_dir=log_dir\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = PPO(\n",
    "    env=env,\n",
    "    **hyperparams,\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_dir,\n",
    "    device='cpu' # PPO is intended to be run on CPU\n",
    ")\n",
    "\n",
    "# Callback for saving checkpoints\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=save_freq,\n",
    "    save_path=log_dir,\n",
    "    name_prefix=\"rl_model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "model.learn(\n",
    "    total_timesteps=n_timesteps,\n",
    "    callback=checkpoint_callback,\n",
    "    tb_log_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "savepath = os.path.join(log_dir, \"ppo_lunarlander_final\")\n",
    "model.save(savepath)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize LunarLandar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(savepath, device='cpu')\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "obs = env.reset()[0]\n",
    "\n",
    "T = 400 # number of timesteps\n",
    "for _ in range(T):\n",
    "    action = model.predict(obs)[0]\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        obs = env.reset()[0]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Cart Pole Balancing Agent (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel environments\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=8)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Set hyperparameters\n",
    "CartPole-v1:\n",
    "  n_envs: 8\n",
    "  n_timesteps: !!float 1e5\n",
    "  policy: 'MlpPolicy'\n",
    "  n_steps: 32\n",
    "  batch_size: 256\n",
    "  gae_lambda: 0.8\n",
    "  gamma: 0.98\n",
    "  n_epochs: 20\n",
    "  ent_coef: 0.0\n",
    "  learning_rate: lin_0.001\n",
    "  clip_range: lin_0.2\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.gamma = 0.98\n",
    "model.gae_lambda = 0.8\n",
    "model.learning_rate = 0.001\n",
    "model.learn(total_timesteps=float(1e5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_cartpole\")\n",
    "obs = vec_env.reset()\n",
    "\n",
    "T = 100 # number of timesteps\n",
    "for _ in range(T):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "vec_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
